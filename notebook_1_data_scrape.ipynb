{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:45.628978Z",
     "start_time": "2020-01-18T21:13:44.166547Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import dateutil.parser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.183137Z",
     "start_time": "2020-01-18T21:13:45.630933Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://www.boxofficemojo.com/yearly/'\n",
    "source_code = requests.get(url)\n",
    "soup = BeautifulSoup(source_code.text, 'lxml')\n",
    "source_code.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:50:12.365321Z",
     "start_time": "2020-01-18T21:50:12.357722Z"
    }
   },
   "outputs": [],
   "source": [
    "#collect urls for the yearly box office charts\n",
    "yearly_chart_pages = []\n",
    "for x in soup.find_all('a'):\n",
    "    if 'chart/?' in x.attrs['href']:\n",
    "        yearly_chart_pages.append(x.attrs['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T22:07:17.039762Z",
     "start_time": "2020-01-18T22:07:17.036304Z"
    }
   },
   "outputs": [],
   "source": [
    "project_years = yearly_chart_pages[1:11] # use most recent 10 complete years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T22:08:04.029844Z",
     "start_time": "2020-01-18T22:07:35.132146Z"
    }
   },
   "outputs": [],
   "source": [
    "#collect soup files for first page results for each year\n",
    "project_year_soups = []\n",
    "for year in project_years:\n",
    "    source = requests.get(url+year)\n",
    "    soup = BeautifulSoup(source.text,'lxml')\n",
    "    project_year_soups.append(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T22:15:12.476787Z",
     "start_time": "2020-01-18T22:15:11.601406Z"
    }
   },
   "outputs": [],
   "source": [
    "#collect links to each movie (top 100 for each of the 10 most recent complete years)\n",
    "movie_links = []\n",
    "for soup in project_year_soups:\n",
    "    for x in soup.find_all('div',id = 'body'):\n",
    "        for y in x.find_all('a'):\n",
    "            for key in y.attrs.keys():\n",
    "                if key == 'href' and '/movies/?' in y.attrs[key]:\n",
    "                    movie_links.append(y.attrs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T22:24:36.509300Z",
     "start_time": "2020-01-18T22:15:37.343743Z"
    }
   },
   "outputs": [],
   "source": [
    "#collect soup files for each movie\n",
    "movie_soups = []\n",
    "for link in movie_links:\n",
    "    source = requests.get('https://www.boxofficemojo.com/'+link)\n",
    "    soup = BeautifulSoup(source.text,'lxml')\n",
    "    movie_soups.append(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.222334Z",
     "start_time": "2020-01-18T21:13:46.217957Z"
    }
   },
   "outputs": [],
   "source": [
    "#save movie_soups\n",
    "with open('movie_soups.pkl','wb') as picklefile:\n",
    "    pickle.dump(movie_soups,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.231792Z",
     "start_time": "2020-01-18T21:13:46.225236Z"
    }
   },
   "outputs": [],
   "source": [
    "#load movie_soups\n",
    "with open(\"movie_soups.pkl\", 'rb') as picklefile: \n",
    "    movie_soups = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.239659Z",
     "start_time": "2020-01-18T21:13:46.233861Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_movie_value(soup, field_name):\n",
    "    \n",
    "    '''Grab a value from Box Office Mojo HTML\n",
    "    \n",
    "    Takes a string attribute of a movie on the page and returns the string in\n",
    "    the next sibling object (the value for that attribute) or None if nothing is found.\n",
    "    '''\n",
    "    \n",
    "    obj = soup.find(text=re.compile(field_name))\n",
    "    \n",
    "    if not obj: \n",
    "        return None\n",
    "    \n",
    "    # this works for most of the values\n",
    "    next_sibling = obj.findNextSibling()\n",
    "    \n",
    "    if next_sibling:\n",
    "        return next_sibling.text \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.250411Z",
     "start_time": "2020-01-18T21:13:46.241533Z"
    }
   },
   "outputs": [],
   "source": [
    "def money_to_int(moneystring): #converts dollar values from strings to int\n",
    "    moneystring = moneystring.replace('$', '').replace(',', '')\n",
    "    return int(moneystring)\n",
    "\n",
    "def runtime_to_minutes(runtimestring): #converts runtime from hrs and min to min\n",
    "    runtime = runtimestring.split()\n",
    "    try:\n",
    "        minutes = int(runtime[0])*60 + int(runtime[2])\n",
    "        return minutes\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def to_date(datestring): #converts date from string to datetime\n",
    "    date = dateutil.parser.parse(datestring)\n",
    "    return date\n",
    "\n",
    "def budget_to_int(budgetstring): #converts budget from string to float\n",
    "    if budgetstring == 'N/A':\n",
    "        budget_int = np.nan\n",
    "    elif 'million' not in budgetstring:\n",
    "        budget_int = float(budgetstring.strip().replace('$','').replace(',',''))\n",
    "    else: \n",
    "        budget_int = budgetstring.split() # separate \n",
    "        budget_int = (float(budget_int[0][1:]) * 1000000)\n",
    "    return budget_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.299139Z",
     "start_time": "2020-01-18T21:13:46.252664Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_movie_data(soup):\n",
    "    '''A function to grab all relevant data points from a movie page on Box Office Mojo\n",
    "        returns a dictionary of field names with corresponding values and a list of errors'''\n",
    "    error_log = []\n",
    "    title = soup.find('title').text.split(' (')[0]\n",
    "    try:\n",
    "        domestic_total_gross = money_to_int(get_movie_value(soup,'Domestic Total Gross'))\n",
    "    except:\n",
    "        domestic_total_gross = np.nan\n",
    "        error_log.append([title,'gross'])\n",
    "    try:\n",
    "        runtime = runtime_to_minutes(get_movie_value(soup,'Runtime'))\n",
    "    except:\n",
    "        runtime = np.nan\n",
    "        error_log.append([title,'runtime'])\n",
    "    try:\n",
    "        budget = budget_to_int(get_movie_value(soup,'Budget'))\n",
    "    except:\n",
    "        budget = np.nan\n",
    "        error_log.append([title,'budget'])\n",
    "    try:\n",
    "        release_date = to_date(get_movie_value(soup,'Release Date'))                \n",
    "    except:\n",
    "        release_date = np.nan\n",
    "        error_log.append([title,'release date'])\n",
    "    try:\n",
    "        genre = get_movie_value(soup,'Genre:')\n",
    "    except:\n",
    "        genre = 'Unknown'\n",
    "        error_log.append([title,'genre'])\n",
    "    try:\n",
    "        mpaa = get_movie_value(soup,'MPAA')\n",
    "    except:\n",
    "        mpaa = 'Unknown'\n",
    "        error_log.append([title,'mpaa'])\n",
    "    try:\n",
    "        distributor = get_movie_value(soup,'Distributor')\n",
    "    except:\n",
    "        distributor = 'Unknown'\n",
    "        error_log.append([title,'distributor'])\n",
    "    if soup.find_all('div',class_ = 'mp_box_content')[1].find_all('td')[-2].text == 'In Release:':\n",
    "        try:\n",
    "            widest_release = soup.find_all('div',class_ = 'mp_box_content')[1].find_all('td')[-5].text.replace('\\xa0','')\n",
    "            widest_release = int(widest_release.split(' ')[0].replace(',',''))\n",
    "        except:\n",
    "            widest_release = np.nan\n",
    "            error_log.append([title,'widest release'])\n",
    "        try:\n",
    "            close_date = soup.find_all('div',class_ = 'mp_box_content')[1].find_all('td')[-3].text.replace('\\xa0','')\n",
    "            close_date = to_date(close_date)\n",
    "        except:\n",
    "            close_date = np.nan\n",
    "            error_log.append([title,'close date'])\n",
    "        try:\n",
    "            days_in_release = soup.find_all('div',class_ = 'mp_box_content')[1].find_all('td')[-1].text.replace('\\xa0','')\n",
    "            days_in_release = int(days_in_release.split(' ')[0])\n",
    "        except:\n",
    "            days_in_release = np.nan\n",
    "            error_log.append([title,'in release'])\n",
    "    else:\n",
    "        days_in_release = np.nan\n",
    "        try:\n",
    "            widest_release = soup.find_all('div',class_ = 'mp_box_content')[1].find_all('td')[-3].text.replace('\\xa0','')\n",
    "            widest_release = int(widest_release.split(' ')[0].replace(',',''))\n",
    "        except:\n",
    "            widest_release = np.nan\n",
    "            error_log.append([title,'widest release'])\n",
    "        try:\n",
    "            close_date = soup.find_all('div',class_ = 'mp_box_content')[1].find_all('td')[-1].text.replace('\\xa0','')\n",
    "            close_date = to_date(close_date)\n",
    "        except:\n",
    "            close_date = np.nan\n",
    "            error_log.append([title,'close date'])\n",
    "    \n",
    "    franchises = []\n",
    "    franchise_links = []\n",
    "    \n",
    "    try:\n",
    "        for f in soup.find('th', text = re.compile('Franchise')).find_parent('tr').find_parent().find_all('a',text = re.compile('Series')):\n",
    "            franchise_link = f.get('href')\n",
    "            franchise = f.text.split('\\n')[0]\n",
    "            franchise = franchise.split(':')[1].strip()\n",
    "            franchises.append(franchise)\n",
    "            franchise_links.append(franchise_link)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    #grab director \n",
    "    try:\n",
    "        director_info = (soup.find('div', class_ = 'mp_box_tab', text = re.compile('The Players'))\n",
    "                         .find_parent().find('td',text = re.compile('Director')).findNextSibling().find('a'))\n",
    "        director = director_info.text.strip()\n",
    "        director_link = director_info.get('href')\n",
    "         \n",
    "    except:\n",
    "        error_log.append([title,'director'])\n",
    "        director_link = ''\n",
    "        director = 'Unknown'\n",
    "    \n",
    "    #grab writer \n",
    "    try:\n",
    "        writer_info = (soup.find('div', class_ = 'mp_box_tab', text = re.compile('The Players'))\n",
    "                       .find_parent().find('td',text = re.compile('Writer')).findNextSibling().find('a'))\n",
    "        writer = writer_info.text.strip()\n",
    "        writer_link = writer_info.get('href')\n",
    "        \n",
    "    except:\n",
    "        error_log.append([title,'writer'])\n",
    "        writer_link = ''\n",
    "        writer = 'Unknown'\n",
    "\n",
    "    #grab actor \n",
    "    try:\n",
    "        actor_info = (soup.find('div', class_ = 'mp_box_tab', text = re.compile('The Players'))\n",
    "                      .find_parent().find('td',text = re.compile('Actor')).findNextSibling().find('a'))\n",
    "        actor = actor_info.text.strip()\n",
    "        actor_link = actor_info.get('href')\n",
    "        \n",
    "    except:\n",
    "        error_log.append([title,'actor'])\n",
    "        actor_link = ''\n",
    "        actor = 'Unknown'\n",
    "\n",
    "    #grab producer \n",
    "    try:\n",
    "        producer_info = (soup.find('div', class_ = 'mp_box_tab', text = re.compile('The Players'))\n",
    "                      .find_parent().find('td',text = re.compile('Producer')).findNextSibling().find('a'))\n",
    "        producer = producer_info.text.strip()\n",
    "        producer_link = producer_info.get('href')\n",
    "        \n",
    "    except:\n",
    "        error_log.append([title,'producer'])\n",
    "        producer_link = ''\n",
    "        producer = 'Unknown'\n",
    "    \n",
    "    headers = (['Title','Domestic Total Gross','Runtime(min)','Budget','Release Date',\n",
    "                'Genre','MPAA','Distributor','Widest Release','Close Date','Days in Release',\n",
    "                'Franchise(s)','Director','Writer','Actor','Producer'])\n",
    "    values = ([title,domestic_total_gross,runtime,budget,release_date,genre,mpaa,distributor,\n",
    "              widest_release,close_date,days_in_release,franchises,director,writer,actor,producer])\n",
    "    movie_dict = dict(zip(headers,values))\n",
    "    links_dict = (dict(zip(['Franchise','Director','Writer','Actor','Producer'],\n",
    "                           [franchise_links,director_link,writer_link,actor_link,producer_link])))\n",
    "    return movie_dict, error_log,links_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.307049Z",
     "start_time": "2020-01-18T21:13:46.301180Z"
    }
   },
   "outputs": [],
   "source": [
    "#save output from get_movie_data function in 3 separate lists\n",
    "movie_data = []\n",
    "errors = []\n",
    "links = []\n",
    "for soup in movie_soups: \n",
    "        index = int(movie_soups.index(soup))\n",
    "        movie_dict, error_log,links_dict = get_movie_data(soup)\n",
    "        movie_data.append(movie_dict)\n",
    "        if len(links_dict) > 0: #only add links_dict to links list if links were extracted from movie page\n",
    "            links.append(links_dict)\n",
    "        if len(error_log) > 0: #only add error_log to errors list if there were errors extracting info from the movie page\n",
    "            error_log.insert(0,index)\n",
    "            errors.append(error_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.313394Z",
     "start_time": "2020-01-18T21:13:46.309229Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_field_links(links_dict,field):\n",
    "    '''returns a list of links for a particular field from the links_dict'''\n",
    "    field_links_list = []\n",
    "    for link in links:\n",
    "        for x in link[field]:\n",
    "            field_links_list.append(x)\n",
    "    return field_links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.319756Z",
     "start_time": "2020-01-18T21:13:46.315801Z"
    }
   },
   "outputs": [],
   "source": [
    "franchise_links = get_field_links(links,'Franchise')\n",
    "franchise_links = list((set(franchise_links))) #removing any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.329192Z",
     "start_time": "2020-01-18T21:13:46.322053Z"
    }
   },
   "outputs": [],
   "source": [
    "#collect names and sizes of franchise from page on boxofficemojo.com\n",
    "url = 'https://www.boxofficemojo.com/'\n",
    "f_names = []\n",
    "f_sizes = []\n",
    "for f in franchise_links:\n",
    "    franchise_source = requests.get(url + f)\n",
    "    franchise_soup = BeautifulSoup(franchise_source.text,'lxml')\n",
    "    franchise_name = franchise_soup.find('h1').text\n",
    "    f_names.append(franchise_name)\n",
    "    franchise_size = (len(franchise_soup.find('td',text = re.compile('Rank')).find_parent()\n",
    "                          .find_parent().find_all('tr')) - 3)\n",
    "    f_sizes.append(franchise_size)\n",
    "\n",
    "franchise_dict = dict(zip(f_names,f_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.336696Z",
     "start_time": "2020-01-18T21:13:46.331167Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_df = pd.DataFrame(movie_data) #create dataframe of movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.431505Z",
     "start_time": "2020-01-18T21:13:46.340378Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time, os\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.443710Z",
     "start_time": "2020-01-18T21:13:46.433802Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_imdb_budget(row):\n",
    "    '''function to scrape budget info from IMBD (was missing from many movie pages on boxofficemojo)'''\n",
    "    if np.isnan(row['Budget']):\n",
    "        try:\n",
    "            title = row['Title']\n",
    "            imdb = \"https://www.imdb.com/\"\n",
    "            driver = webdriver.Chrome(chromedriver)\n",
    "            driver.get(imdb)\n",
    "\n",
    "            search_box = driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "\n",
    "            #clear the current search\n",
    "            search_box.clear()\n",
    "\n",
    "            #input new search\n",
    "            search_box.send_keys(title)\n",
    "\n",
    "            #hit enter\n",
    "            search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "            movie_page_link = soup.find('td',class_ = 'result_text').find('a').get('href')\n",
    "\n",
    "            movie_page = requests.get(imdb + movie_page_link).text\n",
    "            movie_soup = BeautifulSoup(movie_page,'lxml')\n",
    "            budget = (movie_soup.find('h3',text = re.compile('Box Office'))\n",
    "                      .findNextSibling().text.split(':')[1].split('\\n')[0])\n",
    "            budget = budget_to_int(budget)\n",
    "\n",
    "            driver.close()\n",
    "        except:\n",
    "            budget = row['Budget']\n",
    "            driver.close()\n",
    "    else:\n",
    "        budget = row['Budget']\n",
    "    time.sleep(3)\n",
    "    return budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.761538Z",
     "start_time": "2020-01-18T21:13:46.445710Z"
    }
   },
   "outputs": [],
   "source": [
    "#add budget info from IMDB to dataframe\n",
    "movie_df['Budget Adj'] = movie_df.apply(get_imdb_budget,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-18T21:13:46.767572Z",
     "start_time": "2020-01-18T21:13:44.216Z"
    }
   },
   "outputs": [],
   "source": [
    "#save dataframe for modeling\n",
    "with open('movie_df.pkl','wb') as picklefile:\n",
    "    pickle.dump(movie_df,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
